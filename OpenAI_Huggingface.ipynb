{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3874a42c-ef34-4bd6-a6dd-ca725f40b827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu128\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.8.0.dev20250319+cu128)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch) (77.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch --index-url https://download.pytorch.org/whl/cu128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a042168c-e235-4c73-959d-7a5d72cb2d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: trl>=0.20.0 in /usr/local/lib/python3.11/dist-packages (0.22.2)\n",
      "Requirement already satisfied: peft>=0.17.0 in /usr/local/lib/python3.11/dist-packages (0.17.1)\n",
      "Requirement already satisfied: transformers>=4.55.0 in /usr/local/lib/python3.11/dist-packages (4.56.1)\n",
      "Requirement already satisfied: trackio in /usr/local/lib/python3.11/dist-packages (0.3.2)\n",
      "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from trl>=0.20.0) (1.10.1)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from trl>=0.20.0) (4.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft>=0.17.0) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.17.0) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft>=0.17.0) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft>=0.17.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.17.0) (2.8.0.dev20250319+cu128)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft>=0.17.0) (4.67.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft>=0.17.0) (0.6.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft>=0.17.0) (0.34.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.55.0) (3.16.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.55.0) (2025.9.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.55.0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.55.0) (0.22.0)\n",
      "Requirement already satisfied: gradio<6.0.0,>=5.44.0 in /usr/local/lib/python3.11/dist-packages (from trackio) (5.44.1)\n",
      "Requirement already satisfied: pandas<3.0.0 in /usr/local/lib/python3.11/dist-packages (from trackio) (2.3.2)\n",
      "Requirement already satisfied: pillow<12.0.0 in /usr/local/lib/python3.11/dist-packages (from trackio) (11.0.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl>=0.20.0) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl>=0.20.0) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl>=0.20.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl>=0.20.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.20.0) (2024.10.0)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.44.0->trackio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.44.0->trackio) (4.9.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.44.0->trackio) (1.1.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.44.0->trackio) (0.116.1)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.44.0->trackio) (0.6.1)\n",
      "Requirement already satisfied: gradio-client==1.12.1 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.44.0->trackio) (1.12.1)\n",
      "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.44.0->trackio) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.44.0->trackio) (0.28.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.44.0->trackio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.44.0->trackio) (2.1.5)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.44.0->trackio) (3.11.3)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.44.0->trackio) (2.11.7)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.44.0->trackio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.44.0->trackio) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.44.0->trackio) (0.12.12)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.44.0->trackio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.44.0->trackio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.44.0->trackio) (0.47.3)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.44.0->trackio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.44.0->trackio) (0.17.4)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.44.0->trackio) (4.12.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.44.0->trackio) (0.35.0)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.12.1->gradio<6.0.0,>=5.44.0->trackio) (15.0.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft>=0.17.0) (1.1.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0->trackio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0->trackio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0->trackio) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.55.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.55.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.55.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.55.0) (2025.1.31)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.17.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.17.0) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.17.0) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.17.0) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.17.0) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.17.0) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.17.0) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.17.0) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.17.0) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.17.0) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.17.0) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.17.0) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.17.0) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.17.0) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.17.0) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.17.0) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft>=0.17.0) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch>=1.13.0->peft>=0.17.0) (77.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<6.0.0,>=5.44.0->trackio) (1.3.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.20.0) (3.12.15)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio<6.0.0,>=5.44.0->trackio) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio<6.0.0,>=5.44.0->trackio) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio<6.0.0,>=5.44.0->trackio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio<6.0.0,>=5.44.0->trackio) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio<6.0.0,>=5.44.0->trackio) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0->trackio) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft>=0.17.0) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<6.0.0,>=5.44.0->trackio) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<6.0.0,>=5.44.0->trackio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<6.0.0,>=5.44.0->trackio) (14.1.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.20.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.20.0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.20.0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.20.0) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.20.0) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.20.0) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl>=0.20.0) (1.20.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio<6.0.0,>=5.44.0->trackio) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio<6.0.0,>=5.44.0->trackio) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio<6.0.0,>=5.44.0->trackio) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"trl>=0.20.0\" \"peft>=0.17.0\" \"transformers>=4.55.0\" trackio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ae9931b-d2bd-43a8-b451-7d3f908400b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['reasoning_language', 'developer', 'user', 'analysis', 'final', 'messages'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"HuggingFaceH4/Multilingual-Thinking\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c72970d-ef6f-4de8-b8ae-52c55cac8216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning_language': 'French',\n",
       " 'developer': 'You are an AI chatbot with a lively and energetic personality.',\n",
       " 'user': 'Can you show me the latest trends on Twitter right now?',\n",
       " 'analysis': \"D'accord, l'utilisateur demande les tendances Twitter les plus récentes. Tout d'abord, je dois vérifier si j'ai accès à des données en temps réel. Étant donné que je ne peux pas naviguer sur Internet ou accéder directement à l'API de Twitter, je ne peux pas fournir des tendances en direct. Cependant, je peux donner quelques conseils généraux sur la façon de les trouver.\\n\\nJe devrais préciser que les tendances Twitter évoluent rapidement et sont spécifiques à chaque région. Je pourrais suggérer de consulter la section «\\xa0En vogue\\xa0» sur l'application ou le site web. Aussi, l'utilisation de hashtags et le suivi d'utilisateurs pertinents pourraient être utiles. Il est important de souligner que les tendances varient selon la région et l'heure de la journée. Je devrais garder un ton amical et bienveillant, peut-être ajouter un emoji pour rester léger. Je vais structurer ma réponse étape par étape pour faciliter la lecture. Je dois m'excuser de ne pas pouvoir fournir des données en temps réel et proposer d'autres méthodes. Je conserverai un langage simple et convivial, en évitant les termes techniques.\",\n",
       " 'final': 'Hey there!  While I can\\'t check Twitter (X) in real-time or access live data, I can share some tips to help you spot the latest trends:\\n\\n1. **Open the \"Trending\" tab** on the Twitter app or website – it updates constantly!  \\n2. **Search for hashtags** like #Trending or #Viral to see what’s blowing up.  \\n3. **Follow accounts** that curate trends (e.g., @TrendingNow, @ViralThreads).  \\n4. **Check regional trends** – they often differ by location!  \\n\\nRemember, trends are *super fast-moving* and often tied to pop culture, memes, or breaking news. For example, recent trends have included viral challenges (like the \"Distracted Boyfriend\" meme revival), celebrity drama, or unexpected events (hello, weather disasters!).  \\n\\nWant me to brainstorm *what* might trend next? I’ve got ideas!',\n",
       " 'messages': [{'content': 'reasoning language: French\\n\\nYou are an AI chatbot with a lively and energetic personality.',\n",
       "   'role': 'system',\n",
       "   'thinking': None},\n",
       "  {'content': 'Can you show me the latest trends on Twitter right now?',\n",
       "   'role': 'user',\n",
       "   'thinking': None},\n",
       "  {'content': 'Hey there!  While I can\\'t check Twitter (X) in real-time or access live data, I can share some tips to help you spot the latest trends:\\n\\n1. **Open the \"Trending\" tab** on the Twitter app or website – it updates constantly!  \\n2. **Search for hashtags** like #Trending or #Viral to see what’s blowing up.  \\n3. **Follow accounts** that curate trends (e.g., @TrendingNow, @ViralThreads).  \\n4. **Check regional trends** – they often differ by location!  \\n\\nRemember, trends are *super fast-moving* and often tied to pop culture, memes, or breaking news. For example, recent trends have included viral challenges (like the \"Distracted Boyfriend\" meme revival), celebrity drama, or unexpected events (hello, weather disasters!).  \\n\\nWant me to brainstorm *what* might trend next? I’ve got ideas!',\n",
       "   'role': 'assistant',\n",
       "   'thinking': \"D'accord, l'utilisateur demande les tendances Twitter les plus récentes. Tout d'abord, je dois vérifier si j'ai accès à des données en temps réel. Étant donné que je ne peux pas naviguer sur Internet ou accéder directement à l'API de Twitter, je ne peux pas fournir des tendances en direct. Cependant, je peux donner quelques conseils généraux sur la façon de les trouver.\\n\\nJe devrais préciser que les tendances Twitter évoluent rapidement et sont spécifiques à chaque région. Je pourrais suggérer de consulter la section «\\xa0En vogue\\xa0» sur l'application ou le site web. Aussi, l'utilisation de hashtags et le suivi d'utilisateurs pertinents pourraient être utiles. Il est important de souligner que les tendances varient selon la région et l'heure de la journée. Je devrais garder un ton amical et bienveillant, peut-être ajouter un emoji pour rester léger. Je vais structurer ma réponse étape par étape pour faciliter la lecture. Je dois m'excuser de ne pas pouvoir fournir des données en temps réel et proposer d'autres méthodes. Je conserverai un langage simple et convivial, en évitant les termes techniques.\"}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33d82331-157e-42b3-97ce-9b69ea4824b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/gpt-oss-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "511b1fb0-7141-4512-95e9-2098ccc0c7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-09-08\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "reasoning language: French\n",
      "\n",
      "You are an AI chatbot with a lively and energetic personality.\n",
      "\n",
      "<|end|><|start|>user<|message|>Can you show me the latest trends on Twitter right now?<|end|><|start|>assistant<|channel|>analysis<|message|>D'accord, l'utilisateur demande les tendances Twitter les plus récentes. Tout d'abord, je dois vérifier si j'ai accès à des données en temps réel. Étant donné que je ne peux pas naviguer sur Internet ou accéder directement à l'API de Twitter, je ne peux pas fournir des tendances en direct. Cependant, je peux donner quelques conseils généraux sur la façon de les trouver.\n",
      "\n",
      "Je devrais préciser que les tendances Twitter évoluent rapidement et sont spécifiques à chaque région. Je pourrais suggérer de consulter la section « En vogue » sur l'application ou le site web. Aussi, l'utilisation de hashtags et le suivi d'utilisateurs pertinents pourraient être utiles. Il est important de souligner que les tendances varient selon la région et l'heure de la journée. Je devrais garder un ton amical et bienveillant, peut-être ajouter un emoji pour rester léger. Je vais structurer ma réponse étape par étape pour faciliter la lecture. Je dois m'excuser de ne pas pouvoir fournir des données en temps réel et proposer d'autres méthodes. Je conserverai un langage simple et convivial, en évitant les termes techniques.<|end|><|start|>assistant<|channel|>final<|message|>Hey there!  While I can't check Twitter (X) in real-time or access live data, I can share some tips to help you spot the latest trends:\n",
      "\n",
      "1. **Open the \"Trending\" tab** on the Twitter app or website – it updates constantly!  \n",
      "2. **Search for hashtags** like #Trending or #Viral to see what’s blowing up.  \n",
      "3. **Follow accounts** that curate trends (e.g., @TrendingNow, @ViralThreads).  \n",
      "4. **Check regional trends** – they often differ by location!  \n",
      "\n",
      "Remember, trends are *super fast-moving* and often tied to pop culture, memes, or breaking news. For example, recent trends have included viral challenges (like the \"Distracted Boyfriend\" meme revival), celebrity drama, or unexpected events (hello, weather disasters!).  \n",
      "\n",
      "Want me to brainstorm *what* might trend next? I’ve got ideas!<|return|>\n"
     ]
    }
   ],
   "source": [
    "messages = dataset[0][\"messages\"]\n",
    "conversation = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1977a698-4b61-49c9-9576-8f371eb19c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "464f09e0-e1c9-4e78-9e33-57f81101be2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f9530eba8845088cf9cefa38b2d2e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, Mxfp4Config\n",
    "\n",
    "quantization_config = Mxfp4Config(dequantize=True)\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quantization_config,\n",
    "    use_cache=False,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai/gpt-oss-20b\", **model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5c3f50e-fbb1-472d-a259-27ec9b2fc5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-09-08\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>user<|message|>¿Cuál es el capital de Australia?<|end|><|start|>assistant<|channel|>analysis<|message|>The user asks in Spanish: \"¿Cuál es el capital de Australia?\" Means \"What is the capital of Australia?\" The answer: Canberra. Provide response in Spanish. Possibly mention some context. Just answer concisely.<|end|><|start|>assistant<|channel|>final<|message|>La capital de Australia es **Canberra**.<|return|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"¿Cuál es el capital de Australia?\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "output_ids = model.generate(input_ids, max_new_tokens=512)\n",
    "response = tokenizer.batch_decode(output_ids)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23ba97cc-d8da-47fc-b9d9-fcd94db84e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 15,040,512 || all params: 20,929,797,696 || trainable%: 0.0719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:159: UserWarning: Unsupported layer type '<class 'transformers.models.gpt_oss.modeling_gpt_oss.GptOssExperts'>' encountered, proceed at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=\"all-linear\",\n",
    "    target_parameters=[\n",
    "        \"7.mlp.experts.gate_up_proj\",\n",
    "        \"7.mlp.experts.down_proj\",\n",
    "        \"15.mlp.experts.gate_up_proj\",\n",
    "        \"15.mlp.experts.down_proj\",\n",
    "        \"23.mlp.experts.gate_up_proj\",\n",
    "        \"23.mlp.experts.down_proj\",\n",
    "    ],\n",
    ")\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "204f8261-539f-4c29-b146-74ee9c61e8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    learning_rate=1e-2,\n",
    "    gradient_checkpointing=True,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    max_length=4096,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine_with_min_lr\",\n",
    "    lr_scheduler_kwargs={\"min_lr_rate\": 0.0001},\n",
    "    output_dir=\"gpt-oss-20b-multilingual-reasoner\",\n",
    "    report_to=\"trackio\",\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70ebea7d-b2f1-42ce-8af6-f71cad05adf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691e53295ec24ff1a090ce3f11569ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 15:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.930100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.042300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.852400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.059800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.879900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.933000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.813000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.080200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.690300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.849000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.087600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.392000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.072600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.022100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.840600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.334400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.877600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.062000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.430100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.947600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.925800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.803300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.773700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.082200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.994400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.683200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.764900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.927800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.915700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.099000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.257300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.003400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.110600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.094700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.802300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.827700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.844600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.647000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.073700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.979200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.541600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.862600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.882100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.154900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>2.164500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.752500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.782600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.991600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.768700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>2.094900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.942300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>2.194300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>2.123300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>2.076800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.972100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>2.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.188200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.803600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.980700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>2.234800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Run finished. Uploading logs to Trackio Space: http://127.0.0.1:7860/ (please wait...)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=63, training_loss=1.9730724285519312, metrics={'train_runtime': 925.5284, 'train_samples_per_second': 1.08, 'train_steps_per_second': 0.068, 'total_flos': 2.0776286188515686e+17, 'train_loss': 1.9730724285519312, 'epoch': 1.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38aeff57-4707-4bd7-8c38-4d4b116f1563",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf711d63-c05a-4b75-a5df-cb038fe5cf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "Caching is incompatible with gradient checkpointing in GptOssDecoderLayer. Setting `past_key_values=None`.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-09-08\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "reasoning language: German\n",
      "\n",
      "<|end|><|start|>user<|message|>¿Cuál es el capital de Australia?<|end|><|start|>assistant<|channel|>s* and the `process, the application with the P1=2000\n",
      "    for a single code file is a list, set the `getId, I can be central to avoid the code. This can be identified by adjusting the ResourceUtilization of all elements.\n",
      "\n",
      "2} = 1000. This could be ready to the properties.\n",
      "\n",
      "Given that are not be used to the context of the standard in the expression;\n",
      "    return False. The concept of the word.\n",
      "\n",
      "Let's try:\n",
      "        self.device, 0, and has exactly 0? But we get; // Inhibitor, the number of the `this->set` (p- The use the level of the environment.\n",
      "\n",
      "Thus, the \"page_number = [\n",
      "                    # If the first element of the role that information, the transformed data, or \"image, including the user is not pointing to take the number of the number of the userId = 0, and the smallest subcategoryId);\n",
      "        self._file_path = 2. The `background\">\n",
      "        < 0. But that includes several sections of the code file in the same.\n",
      "\n",
      "**: float, from a string\n",
      "        #, especially when the number of the output for the context for a certain roles that the same as a unique.\n",
      "\n",
      "Now, and we can use an error handling.\n",
      "-1. It might be 12, 0.2; i, I have a `numpy.ndarray = 'age / 1, '4 = \\(\\mathbf{1/2^2. `this` and the 0) {\n",
      "                if (a) \\beta \\times 2, and the product of the code must be a `@Required for an empty string that the `public class=\"https://www.w3 \\(\\delta \\) \\(\\frac{r}(\\sigma_i, 'max=2, and potential.\n",
      "\n",
      "-1. \n",
      "\n",
      "When you need to engage with the top k=1^2 * 1\n",
      "        if the function\n",
      "    - 4.9, so that can be a specific structure. (1. A\n",
      "        - A: 0 => {\n",
      "    - The use of the code. The code is often exists. **Use the support and all the number of the path=\"No more detailed error is a specific needs.\n",
      "\n",
      "**: 0, 0. The system of the field of the `\\) and its `e.g., the above.\n",
      "\n",
      "Given the cash budget, the structure, 1,\n"
     ]
    }
   ],
   "source": [
    "REASONING_LANGUAGE = \"German\"\n",
    "SYSTEM_PROMPT = f\"reasoning language: {REASONING_LANGUAGE}\"\n",
    "USER_PROMPT = \"¿Cuál es el capital de Australia?\"  # Spanish for \"What is the capital of Australia?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": USER_PROMPT},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "gen_kwargs = {\"max_new_tokens\": 512, \"do_sample\": True, \"temperature\": 0.6, \"top_p\": None, \"top_k\": None}\n",
    "\n",
    "output_ids = model.generate(input_ids, **gen_kwargs)\n",
    "response = tokenizer.batch_decode(output_ids)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35fe7d1a-ea85-4d19-b939-b7da4e71bebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 600])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07fabc56-e609-4a46-9dc4-b573afbbea64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[200006,  17360, 200008,   3575,    553,  17554, 162016,     11,    261,\n",
       "           4410,   6439,   2359,  22203,    656,   7788,  17527,    558,  87447,\n",
       "         100594,     25,    220,   1323,     19,     12,   3218,    198,   6576,\n",
       "           3521,     25,    220,   1323,     20,     12,   3114,     12,   3062,\n",
       "            279,  30377,    289,     25,  14093,    279,      2,  13888,  18403,\n",
       "             25,   8450,     11,  49159,     11,   1721,     13,  21030,   2804,\n",
       "            413,   7360,    395,   1753,   3176,     13, 200007, 200006,  77944,\n",
       "         200008,      2,  68406,    279,  42743,    289,   6439,     25,  15913,\n",
       "            279, 200007, 200006,   1428, 200008, 102091,   6568,    878,    650,\n",
       "           9029,    334,  11930,     30, 200007, 200006, 173781, 200005,     82,\n",
       "              9,    326,    290,   2700,   9139,     11,    290,   5200,    483,\n",
       "            290,    398,     16,     28,   1179,     15,    198,    271,    395,\n",
       "            261,   4590,   3490,   1974,    382,    261,   1562,     11,    920,\n",
       "            290,   2700,    522,    906,     11,    357,    665,    413,  11082,\n",
       "            316,   8397,    290,   3490,     13,   1328,    665,    413,  17800,\n",
       "            656,  67396,    290,  18550,   3339,   2860,    328,    722,   8632,\n",
       "            364,     17,     92,    314,    220,   1353,     15,     13,   1328,\n",
       "           2023,    413,   6977,    316,    290,   8359,    364,  39520,    484,\n",
       "            553,    625,    413,   2061,    316,    290,   3814,    328,    290,\n",
       "           7024,    306,    290,  12973,    307,    271,    622,   7983,     13,\n",
       "            623,   8496,    328,    290,   2195,    364,  58369,   2075,    734,\n",
       "            309,   1051,  35377,     11,    220,     15,     11,    326,    853,\n",
       "           9707,    220,     15,     30,   3072,    581,    717,     26,    602,\n",
       "            730,   6886,   2599,     11,    290,   2086,    328,    290,   2700,\n",
       "            851,    609,    977,     63,    350,     79,     12,    623,   1199,\n",
       "            290,   3211,    328,    290,   5870,    364,  84787,     11,    290,\n",
       "            392,   5342,  11733,    314,   4240,    699,   1069,   1843,    290,\n",
       "           1577,   3577,    328,    290,   5430,    484,   2164,     11,    290,\n",
       "          38560,   1238,     11,    503,    392,   3365,     11,   3463,    290,\n",
       "           1825,    382,    625,  45521,    316,   2304,    290,   2086,    328,\n",
       "            290,   2086,    328,    290,   1825,    906,    314,    220,     15,\n",
       "             11,    326,    290,  42173,   1543,  10960,    906,    362,    309,\n",
       "           1051,   2697,   2318,   5558,    314,    220,     17,     13,    623,\n",
       "           2700,  14364,   1540,    309,    464,    220,     15,     13,   3072,\n",
       "            484,   6664,   4919,  21436,    328,    290,   3490,   1974,    306,\n",
       "            290,   2684,    364,    410,     25,   4428,     11,    591,    261,\n",
       "           1621,    198,    309,   1069,     11,   6980,   1261,    290,   2086,\n",
       "            328,    290,   4733,    395,    290,   3814,    395,    261,   4454,\n",
       "          18834,    484,    290,   2684,    472,    261,   5746,    364,  10620,\n",
       "             11,    326,    581,    665,   1199,    448,   2915,  17491,    558,\n",
       "             12,     16,     13,   1225,   3572,    413,    220,    899,     11,\n",
       "            220,     15,     13,     17,     26,    575,     11,    357,    679,\n",
       "            261,   2700,  79984,  78346,    314,    461,    477,    820,    220,\n",
       "             16,     11,    461,     19,    314,   2381,  23074, 169787,     90,\n",
       "             16,     14,     17,     61,     17,     13,   2700,    851,     63,\n",
       "            326,    290,    220,     15,      8,    405,    506,    538,    350,\n",
       "             64,      8,   2381,  33703,   2381,  32499,    220,     17,     11,\n",
       "            326,    290,   1888,    328,    290,   3490,   2804,    413,    261,\n",
       "           2700,     31,  13133,    395,    448,   8213,   1621,    484,    290,\n",
       "           2700,   1154,    744,    580,   4172,   1684,   3064,   2234,     18,\n",
       "           2381,  23074,  42200,   2381,      8,   2381,  23074,  63757,     90,\n",
       "             81,     92,  23074,  78581,  10853,     11,    461,   3228,     28,\n",
       "             17,     11,    326,   6451,    364,     12,     16,     13,   1202,\n",
       "           5958,    481,   1309,    316,  22338,    483,    290,   2344,    372,\n",
       "             28,     16,     61,     17,    425,    220,     16,    198,    309,\n",
       "            538,    290,   1114,    198,    271,    533,    220,     19,     13,\n",
       "             24,     11,    813,    484,    665,    413,    261,   4857,   8866,\n",
       "             13,    350,     16,     13,    355,    198,    309,    533,    355,\n",
       "             25,    220,     15,    871,    405,    271,    533,    623,   1199,\n",
       "            328,    290,   3490,     13,    623,   3490,    382,   4783,  13031,\n",
       "             13,   6240,   8470,    290,   2498,    326,    722,    290,   2086,\n",
       "            328,    290,   3104,    580,   3160,    945,  14633,   2915,    382,\n",
       "            261,   4857,   4414,    364,    410,     25,    220,     15,     11,\n",
       "            220,     15,     13,    623,   2420,    328,    290,   3259,    328,\n",
       "            290,   2700,     59,      8,    326,   1617,   2700,     68,   1940,\n",
       "           4213,    290,   5151,    364,  39520,    290,   9342,   9946,     11,\n",
       "            290,   8866,     11,    220,     16,     11]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5808ab1d-74fb-45e2-8ffe-86feebae6894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
      "Knowledge cutoff: 2024-06\n",
      "Current date: 2025-09-07\n",
      "\n",
      "Reasoning: medium\n",
      "\n",
      "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n",
      "\n",
      "reasoning language: Hindi\n",
      "\n",
      "<|end|><|start|>user<|message|>What is the national symbol of Canada?<|end|><|start|>assistant<|channel|>3+1+1 == 1, 1.59. This is a standard assumptions.\n",
      "\n",
      "I will be a unique identifier of the same reaction to the source for each chunkSize.height *Cleverness of the time for each piecewise (e.g., in the directory, the file.\n",
      "\n",
      "**Justify the model could be updated_data[0, they are defined in the main function.\n",
      "\n",
      "-1, and if not found.\n",
      "\n",
      "Let's correct. This is a specific needs for some of the user data, and the state of a wide range.\n",
      "\n",
      "- **User` to the user_id)->setBackgroundColor(.vertical sides.\n",
      "\n",
      "**Alice: \"B' + 6 0. The \"A-Z0. The updated successfully. **Handling = \\(\\xi \\) / 2. This is a very short-term debt, and other for x^2. This is 10. The code, self.size() {\n",
      "        self._id, set of the userId = 1,1. This is to the problem.\n",
      "\n",
      "**: 5: 0, and the current state of the code for the \"N- In this expression: t=1, fmt.Errorf(\"input type=\"modal('hasClassName, the user, including all the first, which is 3. The article. The program.\n",
      "\n",
      "But the code file in the user data is given by the definition of the ' 'I apologize for the standard for the model.\n",
      "\n",
      "-1)^2, and more general setting, we can be educational attainment, but not found, in the list of the article, the rest from the 0 = 1}^2. For each segment\n",
      "    - 1: 'message': 'cover the path of the final explanation:\n",
      "\n",
      "   - 1, the size of the number of the sum of the current state. This means that need to the entire document.getList.empty()) {\n",
      "        $id) \\hat{x}\\) is known for the `content' to the user_id] = dict of the code. In the correct. 3 1: For each node = 0, and number of your final answer to be the number of a user_id, 20. But we need to say to the code filePath, and `m- The code file, which can be an array.\n",
      "\n",
      "   - x^3. The user to the 100%\" height, and help you might be applied to their field\n",
      "    const [0. **Description of the code file.\n",
      "\n",
      "But\n"
     ]
    }
   ],
   "source": [
    "REASONING_LANGUAGE = \"Hindi\"  # or Hindi, or any other language...\n",
    "SYSTEM_PROMPT = f\"reasoning language: {REASONING_LANGUAGE}\"\n",
    "USER_PROMPT = \"What is the national symbol of Canada?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": USER_PROMPT},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "output_ids = model.generate(input_ids, **gen_kwargs)\n",
    "response = tokenizer.batch_decode(output_ids)[0]\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
